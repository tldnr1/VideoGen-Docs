# Index
* 모델 개요
    - Z-Image
    - Qwen-Image/Edit
    - Wan2.2
* AWS 서빙
    - VRAM 요약
    - S2V 구동 예시

<br>
<br>

# 모델 개요
## Z-Image
### Z-Image-Turbo
- Params: 6B
- VRAM: 16G 권장 (pure c++ 구동 시 4GB까지 가능은 하다고 함) 
- Architecture: S3-Dit
- Z-Image distilled 모델로 T2I를 지원. 경량 모델로 빠른 이미지 생성에 초점.

### Z-Image-Base
- non-distilled, 추후 릴리즈 예정

### Z-Image-Edit
- Z-Image에서 edit fine-tuned 버전, 추후 릴리즈 예정

<br>
<br>

## Qwen
- Params: 20B
- Architecture: MMDit

### Qwen-Image (text-to-image)
- [ComfyUI Docs - for Qwen](https://docs.comfy.org/tutorials/image/qwen/qwen-image)
- 이미지 생성용
- 40 ~ 45GB 요구됨

### Qwen-Image-Edit
- 단일 이미지 편집용
- base와 달리 Qwen2.5-VL(텍스트, 이미지 분석용), VAE Encoder가 필요하여 추가 gpu 사용됨
- 약 60GB 이상 소모되는 것으로 보임
- 일관성 유지에 한계를 보인다고 함

### Qwen-Image-Edit-2509
- 다중 이미지 편집용
- 일관성 유지 개선 버전
- person + person, scene, object 다 지원되는 버전이라고 함
- 기본 edit 보다 input 처리를 위해 추가 용량 소모됨

<br>
<br>

## Wan2.2
- [ComfyUI Docs - for Wan](https://docs.comfy.org/tutorials/video/wan/wan2_2)
- [Prompt Recipe - from AliDocs](https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y)
- T2V, I2V, TI2V, S2V, Animate Character(V2V) 버전 존재
- 중심은 Wan2.2-TI2V-5B / Wan2.2-A14B 2가지 모델

### 1. Wan2.2-TI2V-5B
- Params: 5B
- VRAM: 24GB 권장 / 최적화 시 16GB 가능하다고 함
- Architecture: Standard Dit + 고압축 VAE
- 720p 급 영상을 효율적인 가격으로 생성하는 데에 초점

### 2. Wan2.2-S2V-14B (Speech-to-Video)
#### 2-1) 기본 개요
- Params: 14B Active (MoE 특성 상 Full-Size 27B 모두 VRAM에 올려야 함)
- VRAM: 80GB 권장
- Architecture: MoE
- MoE 특성 상 요구량이 높아 H100 이상에서 원활한 구동을 하는 것이 권장됨

#### 2-2) 특징
- Image-to-Video에 Speech가 더해진 형태
- 주로 Lip-sync와 표정을 맞추는 것에 초점 (주요 타겟 범위가 Head & Shoulder 영역)
- 음성(입모양 제어) > pose > text 순으로 제어 강도에 우선순위를 두는 것으로 보임 (말하는 음성에 말하지 말라고 텍스트 입력 시 적용되지 않는다고 함)

#### 2-3) 한계
- 전신 움직임 생성: comfyui docs 기준 전신에 대해 생성 가능하다고는 함. Animate 모델이 춤 같은 동작에 대해서는 더 잘 지원할 것으로 보임
- 새로운 앵글 창조 한계: input에 없는 범위에 대해 생성 한계 (정면 이미지에 대해 뒤로 도는 행동 등)
- 극단적인 조명 변경 불가: 밤 시간대 이미지에 대낮 요청 시 불완전한 결과 보임

### 3. Wan2.2-Animate-14B (Video-to-Video)
- Move mode: 내 캐릭터 사진 + 다른 동작 영상 => 내 캐릭터가 동작 따라하는 영상 생성
- Mix mode: 영화 장면 + 내 캐릭터 사진 => 영화 캐릭터가 내 캐릭터로 교체된 영상 생성

<br>
<br>

# AWS 서빙
* [p5.48xlarge EC2 Price](https://aws-pricing.com/p5.48xlarge.html)

### 모델 별 요구되는 VRAM 요약
- Z-Image: 16GB
- Qwen-Image/Edit: ~45GB/~60GB => H100 80GB 1개 이상 필요
- Wan2.2-S2V-14B: ~60GB(single)/~40GB(4or8 multi-gpu) => p5.48xlarge 환경에서 4or8 병렬 연결 권장됨
    - 최소 사양으로 p4d.24xlarge (A100 40GB x8)가 가능은 하나, Peak VRAM이 37GB로 OOM 가능성이 있어 권장하지 않는 것으로 보임

### 구동 예시 (S2V)
- multi gpu 버전
``` python
torchrun --nproc_per_node=8 generate.py \
    --task s2v-14B \
    --size 1024*704 \
    --ckpt_dir ./Wan2.2-S2V-14B/ \
    --dit_fsdp \
    --t5_fsdp \
    --ulysses_size 8 \
    --prompt "a person is singing" \
    --image "examples/pose.png" \
    --audio "examples/sing.MP3" \
    --pose_video "./examples/pose.mp4"
```

- diffusers 버전: [참고링크](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers)